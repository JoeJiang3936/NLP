{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_SQuAD1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoeJiang3936/NLP/blob/master/Bert_SQuAD1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBcMIY_mJZeN",
        "colab_type": "code",
        "outputId": "0f7f21ce-3b4b-4095-9dfd-fe72afa5c7ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 2.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.9)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.3.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.10.40)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.13.40)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=45e25e7d19de18d76d74ca9e2da1dce816a448d50e4b523a7483bdfb1995aa75\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.35 sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v383Ox4GNRcx",
        "colab_type": "code",
        "outputId": "cae396bc-9536-45e0-c1b2-022dd50fae5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.40)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.40)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (2.6.1)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zohPhxfeUpBV",
        "colab_type": "code",
        "outputId": "3fb111ce-65d9-4cda-d6d7-67a0fea72f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=dc547d6193932d41d19e2aa432b2693291fadf2495c145e22942f4a3cf7062a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho1PLx7uq0l7",
        "colab_type": "code",
        "outputId": "b9aaee7f-321f-4b50-a2cc-26fc998041d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import wget\n",
        "wget.download(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
        "wget.download(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
        "wget.download(\"https://raw.githubusercontent.com/allenai/bi-att-flow/master/squad/evaluate-v1.1.py\")\n",
        "wget.download(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\")\n",
        "wget.download('https://raw.githubusercontent.com/nlpyang/pytorch-transformers/master/examples/utils_squad.py')\n",
        "wget.download('https://raw.githubusercontent.com/nlpyang/pytorch-transformers/master/examples/utils_squad_evaluate.py')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'utils_squad_evaluate.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTN96UpOUvlv",
        "colab_type": "code",
        "outputId": "70393955-36e2-4481-9d7c-4f44f0df8f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuD7r3plDD2B",
        "colab_type": "code",
        "outputId": "b54c618a-fcb2-48da-85dc-f4ee434bc7cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
        "  print(\"We will use GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print('There are no GPU available, using the CPU instead.')\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huN8_5KwUmW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from pytorch_transformers import BertForQuestionAnswering, BertTokenizer\n",
        "from utils_squad import read_squad_examples, convert_examples_to_features, RawResult, write_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYxP5pyBQPNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 2\n",
        "batch_size = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ-2qcgmQPPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEHqQlAgh9qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxrg0Nk9tpSE",
        "colab_type": "code",
        "outputId": "6ec6bb80-d276-4be9-881e-ccf14eb3844f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#initialize Bert base model \n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 161319.38B/s]\n",
            "100%|██████████| 440473133/440473133 [00:37<00:00, 11610054.18B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmQLB7jfQPRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = './train-v1.1.json'\n",
        "train_examples = read_squad_examples(train_file, is_training = True, version_2_with_negative=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LnGqZY1QPTw",
        "colab_type": "code",
        "outputId": "45fb0a0b-98d1-40f8-bac7-9924f950a06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_examples)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87599"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV9BlrzJkSVD",
        "colab_type": "code",
        "outputId": "de7b5093-75fe-452a-dabd-eb806a17624d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "train_examples[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "qas_id: 5733be284776f41900661182, question_text: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?, doc_tokens: [Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.], start_position: 90, end_position: 92"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udk4bjXAQPWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenize the training examples\n",
        "tokenizer = BertTokenizer(vocab_file='bert-base-uncased-vocab.txt')\n",
        "train_features = convert_examples_to_features(train_examples, tokenizer, max_seq_length=384, doc_stride=128, max_query_length=64, is_training=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_FaOS7yQPYw",
        "colab_type": "code",
        "outputId": "3bce3f4f-f86c-44a7-dbd3-75bd39215478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_features)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "033mb0-ltokh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the tokenized data into torch tensors\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype = torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype = torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype = torch.long)\n",
        "all_start_positions = torch.tensor([f.start_position for f in train_features], dtype = torch.long)\n",
        "all_end_positions = torch.tensor([f.end_position for f in train_features], dtype = torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIiqXQvBtovf",
        "colab_type": "code",
        "outputId": "f44dd7a4-a292-4cf9-9313-da2377153304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "all_input_ids[1], all_input_mask[1], all_segment_ids[1], all_start_positions[1], all_end_positions[1]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  101,  2054,  2003,  1999,  2392,  1997,  1996, 10289,  8214,  2364,\n",
              "          2311,  1029,   102,  6549,  2135,  1010,  1996,  2082,  2038,  1037,\n",
              "          3234,  2839,  1012, 10234,  1996,  2364,  2311,  1005,  1055,  2751,\n",
              "          8514,  2003,  1037,  3585,  6231,  1997,  1996,  6261,  2984,  1012,\n",
              "          3202,  1999,  2392,  1997,  1996,  2364,  2311,  1998,  5307,  2009,\n",
              "          1010,  2003,  1037,  6967,  6231,  1997,  4828,  2007,  2608,  2039,\n",
              "         14995,  6924,  2007,  1996,  5722,  1000,  2310,  3490,  2618,  4748,\n",
              "          2033, 18168,  5267,  1000,  1012,  2279,  2000,  1996,  2364,  2311,\n",
              "          2003,  1996, 13546,  1997,  1996,  6730,  2540,  1012,  3202,  2369,\n",
              "          1996, 13546,  2003,  1996, 24665, 23052,  1010,  1037, 14042,  2173,\n",
              "          1997,  7083,  1998,  9185,  1012,  2009,  2003,  1037, 15059,  1997,\n",
              "          1996, 24665, 23052,  2012, 10223, 26371,  1010,  2605,  2073,  1996,\n",
              "          6261,  2984, 22353,  2135,  2596,  2000,  3002, 16595,  9648,  4674,\n",
              "          2061, 12083,  9711,  2271,  1999,  8517,  1012,  2012,  1996,  2203,\n",
              "          1997,  1996,  2364,  3298,  1006,  1998,  1999,  1037,  3622,  2240,\n",
              "          2008,  8539,  2083,  1017, 11342,  1998,  1996,  2751,  8514,  1007,\n",
              "          1010,  2003,  1037,  3722,  1010,  2715,  2962,  6231,  1997,  2984,\n",
              "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor(52),\n",
              " tensor(56))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHo2dfjBtpCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_start_positions, all_end_positions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qI4EQCttpK2",
        "colab_type": "code",
        "outputId": "99ddaceb-ce2e-4f8e-c75e-3218303e48be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_data[1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  101,  2054,  2003,  1999,  2392,  1997,  1996, 10289,  8214,  2364,\n",
              "          2311,  1029,   102,  6549,  2135,  1010,  1996,  2082,  2038,  1037,\n",
              "          3234,  2839,  1012, 10234,  1996,  2364,  2311,  1005,  1055,  2751,\n",
              "          8514,  2003,  1037,  3585,  6231,  1997,  1996,  6261,  2984,  1012,\n",
              "          3202,  1999,  2392,  1997,  1996,  2364,  2311,  1998,  5307,  2009,\n",
              "          1010,  2003,  1037,  6967,  6231,  1997,  4828,  2007,  2608,  2039,\n",
              "         14995,  6924,  2007,  1996,  5722,  1000,  2310,  3490,  2618,  4748,\n",
              "          2033, 18168,  5267,  1000,  1012,  2279,  2000,  1996,  2364,  2311,\n",
              "          2003,  1996, 13546,  1997,  1996,  6730,  2540,  1012,  3202,  2369,\n",
              "          1996, 13546,  2003,  1996, 24665, 23052,  1010,  1037, 14042,  2173,\n",
              "          1997,  7083,  1998,  9185,  1012,  2009,  2003,  1037, 15059,  1997,\n",
              "          1996, 24665, 23052,  2012, 10223, 26371,  1010,  2605,  2073,  1996,\n",
              "          6261,  2984, 22353,  2135,  2596,  2000,  3002, 16595,  9648,  4674,\n",
              "          2061, 12083,  9711,  2271,  1999,  8517,  1012,  2012,  1996,  2203,\n",
              "          1997,  1996,  2364,  3298,  1006,  1998,  1999,  1037,  3622,  2240,\n",
              "          2008,  8539,  2083,  1017, 11342,  1998,  1996,  2751,  8514,  1007,\n",
              "          1010,  2003,  1037,  3722,  1010,  2715,  2962,  6231,  1997,  2984,\n",
              "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor(52),\n",
              " tensor(56))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PFXcXUztpOl",
        "colab_type": "code",
        "outputId": "f03ff2a8-da73-40c0-dcdb-5a7fb9506873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "len(train_dataloader)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5541"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjvCyujPYN50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2dbf029d-0001-40d4-ffb2-a92280cbc263"
      },
      "source": [
        "dev_file = './dev-v1.1.json'\n",
        "dev_examples = read_squad_examples(dev_file, is_training = False, version_2_with_negative=False)\n",
        "\n",
        "print(\"total evaluation samples = \", len(dev_examples))\n",
        "dev_examples[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total evaluation samples =  10570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "qas_id: 56be4db0acb8001400a502ec, question_text: Which NFL team represented the AFC at Super Bowl 50?, doc_tokens: [Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odnq6F7mYOGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "096ccc4c-2371-4b04-cbaa-e79af63da9a4"
      },
      "source": [
        "dev_features = convert_examples_to_features(dev_examples, tokenizer, max_seq_length=384, doc_stride=128, max_query_length=64, is_training=False)\n",
        "len(dev_features)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10833"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdlzZoI8YOPN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "205493ef-e519-47d0-832c-403b91530ff2"
      },
      "source": [
        "# convert dev tokenized data into torch tensors\n",
        "all_input_ids = torch.tensor([f.input_ids for f in dev_features], dtype = torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in dev_features], dtype = torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in dev_features], dtype = torch.long)\n",
        "all_example_index = torch.arange(all_input_ids.size(0), dtype = torch.long)\n",
        "\n",
        "all_input_ids[1], all_input_mask[1], all_segment_ids[1], all_example_index[1]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  101,  2029,  5088,  2136,  3421,  1996, 22309,  2012,  3565,  4605,\n",
              "          2753,  1029,   102,  3565,  4605,  2753,  2001,  2019,  2137,  2374,\n",
              "          2208,  2000,  5646,  1996,  3410,  1997,  1996,  2120,  2374,  2223,\n",
              "          1006,  5088,  1007,  2005,  1996,  2325,  2161,  1012,  1996,  2137,\n",
              "          2374,  3034,  1006, 10511,  1007,  3410,  7573, 14169,  3249,  1996,\n",
              "          2120,  2374,  3034,  1006, 22309,  1007,  3410,  3792, 12915,  2484,\n",
              "          1516,  2184,  2000,  7796,  2037,  2353,  3565,  4605,  2516,  1012,\n",
              "          1996,  2208,  2001,  2209,  2006,  2337,  1021,  1010,  2355,  1010,\n",
              "          2012, 11902,  1005,  1055,  3346,  1999,  1996,  2624,  3799,  3016,\n",
              "          2181,  2012,  4203, 10254,  1010,  2662,  1012,  2004,  2023,  2001,\n",
              "          1996, 12951,  3565,  4605,  1010,  1996,  2223, 13155,  1996,  1000,\n",
              "          3585,  5315,  1000,  2007,  2536,  2751,  1011, 11773, 11107,  1010,\n",
              "          2004,  2092,  2004,  8184, 28324,  2075,  1996,  4535,  1997, 10324,\n",
              "          2169,  3565,  4605,  2208,  2007,  3142, 16371, 28990,  2015,  1006,\n",
              "          2104,  2029,  1996,  2208,  2052,  2031,  2042,  2124,  2004,  1000,\n",
              "          3565,  4605,  1048,  1000,  1007,  1010,  2061,  2008,  1996,  8154,\n",
              "          2071, 14500,  3444,  1996,  5640, 16371, 28990,  2015,  2753,  1012,\n",
              "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor(1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23zJykJpYOlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b850cbaf-a727-4ce2-ef57-276ba0d451a8"
      },
      "source": [
        "dev_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
        "len(dev_dataloader)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "678"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhjK9ihztpVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set up the optimizer\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "optimizer = AdamW(model.parameters(), lr = 3e-5, eps = 1e-4, correct_bias=False)\n",
        "total_steps = len(train_dataloader)*epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8N2I7H7yT-7",
        "colab_type": "code",
        "outputId": "14d4ce50-c9b9-4c51-8a2b-3b8a8d136776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time \n",
        "\n",
        "print('Training the model ...')\n",
        "time0 = time.time()\n",
        "lossList = []\n",
        "for epoch in range(4):  \n",
        "  total_loss = 0\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    model.train()\n",
        "    if step%40 == 0 and not step == 0:\n",
        "      elapsed = time.time() - time0\n",
        "      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "   \n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
        "    outputs = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
        "    loss = outputs[0]\n",
        "    total_loss += loss\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    model.zero_grad()\n",
        "  avg_train_loss = (total_loss/len(train_dataloader)).detach().cpu().numpy()\n",
        "  lossList.append(avg_train_loss)\n",
        "  print('training time for epoch: ', epoch, ': ', time.time()-time0)\n",
        "  print('Average loss after epoch ', epoch, ': ', avg_train_loss)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model ...\n",
            "  Batch    40  of  5,541.    Elapsed: 27.935028314590454.\n",
            "  Batch    80  of  5,541.    Elapsed: 55.86741089820862.\n",
            "  Batch   120  of  5,541.    Elapsed: 83.76790237426758.\n",
            "  Batch   160  of  5,541.    Elapsed: 111.66671466827393.\n",
            "  Batch   200  of  5,541.    Elapsed: 139.54198908805847.\n",
            "  Batch   240  of  5,541.    Elapsed: 167.45866084098816.\n",
            "  Batch   280  of  5,541.    Elapsed: 195.34645342826843.\n",
            "  Batch   320  of  5,541.    Elapsed: 223.24408769607544.\n",
            "  Batch   360  of  5,541.    Elapsed: 251.1399393081665.\n",
            "  Batch   400  of  5,541.    Elapsed: 278.9976963996887.\n",
            "  Batch   440  of  5,541.    Elapsed: 306.89480996131897.\n",
            "  Batch   480  of  5,541.    Elapsed: 334.76584935188293.\n",
            "  Batch   520  of  5,541.    Elapsed: 362.68334102630615.\n",
            "  Batch   560  of  5,541.    Elapsed: 390.5684218406677.\n",
            "  Batch   600  of  5,541.    Elapsed: 418.4500529766083.\n",
            "  Batch   640  of  5,541.    Elapsed: 446.3156006336212.\n",
            "  Batch   680  of  5,541.    Elapsed: 474.19629192352295.\n",
            "  Batch   720  of  5,541.    Elapsed: 502.0587191581726.\n",
            "  Batch   760  of  5,541.    Elapsed: 529.9429619312286.\n",
            "  Batch   800  of  5,541.    Elapsed: 557.7837443351746.\n",
            "  Batch   840  of  5,541.    Elapsed: 585.6576199531555.\n",
            "  Batch   880  of  5,541.    Elapsed: 613.5227494239807.\n",
            "  Batch   920  of  5,541.    Elapsed: 641.4194824695587.\n",
            "  Batch   960  of  5,541.    Elapsed: 669.2964034080505.\n",
            "  Batch 1,000  of  5,541.    Elapsed: 697.1895446777344.\n",
            "  Batch 1,040  of  5,541.    Elapsed: 725.0679380893707.\n",
            "  Batch 1,080  of  5,541.    Elapsed: 752.9309222698212.\n",
            "  Batch 1,120  of  5,541.    Elapsed: 780.812002658844.\n",
            "  Batch 1,160  of  5,541.    Elapsed: 808.7010736465454.\n",
            "  Batch 1,200  of  5,541.    Elapsed: 836.5662603378296.\n",
            "  Batch 1,240  of  5,541.    Elapsed: 864.4394965171814.\n",
            "  Batch 1,280  of  5,541.    Elapsed: 892.3081586360931.\n",
            "  Batch 1,320  of  5,541.    Elapsed: 920.1870169639587.\n",
            "  Batch 1,360  of  5,541.    Elapsed: 948.0862526893616.\n",
            "  Batch 1,400  of  5,541.    Elapsed: 975.9273738861084.\n",
            "  Batch 1,440  of  5,541.    Elapsed: 1003.8088994026184.\n",
            "  Batch 1,480  of  5,541.    Elapsed: 1031.6928601264954.\n",
            "  Batch 1,520  of  5,541.    Elapsed: 1059.5832433700562.\n",
            "  Batch 1,560  of  5,541.    Elapsed: 1087.4480350017548.\n",
            "  Batch 1,600  of  5,541.    Elapsed: 1115.2983872890472.\n",
            "  Batch 1,640  of  5,541.    Elapsed: 1143.1562452316284.\n",
            "  Batch 1,680  of  5,541.    Elapsed: 1171.0035982131958.\n",
            "  Batch 1,720  of  5,541.    Elapsed: 1198.8434567451477.\n",
            "  Batch 1,760  of  5,541.    Elapsed: 1226.6836347579956.\n",
            "  Batch 1,800  of  5,541.    Elapsed: 1254.528615474701.\n",
            "  Batch 1,840  of  5,541.    Elapsed: 1282.4055337905884.\n",
            "  Batch 1,880  of  5,541.    Elapsed: 1310.2764060497284.\n",
            "  Batch 1,920  of  5,541.    Elapsed: 1338.1634118556976.\n",
            "  Batch 1,960  of  5,541.    Elapsed: 1366.0103178024292.\n",
            "  Batch 2,000  of  5,541.    Elapsed: 1393.9007506370544.\n",
            "  Batch 2,040  of  5,541.    Elapsed: 1421.7755501270294.\n",
            "  Batch 2,080  of  5,541.    Elapsed: 1449.6356418132782.\n",
            "  Batch 2,120  of  5,541.    Elapsed: 1477.4876339435577.\n",
            "  Batch 2,160  of  5,541.    Elapsed: 1505.3306365013123.\n",
            "  Batch 2,200  of  5,541.    Elapsed: 1533.1712446212769.\n",
            "  Batch 2,240  of  5,541.    Elapsed: 1561.0487217903137.\n",
            "  Batch 2,280  of  5,541.    Elapsed: 1588.8910377025604.\n",
            "  Batch 2,320  of  5,541.    Elapsed: 1616.7578573226929.\n",
            "  Batch 2,360  of  5,541.    Elapsed: 1644.5884456634521.\n",
            "  Batch 2,400  of  5,541.    Elapsed: 1672.4358398914337.\n",
            "  Batch 2,440  of  5,541.    Elapsed: 1700.280590057373.\n",
            "  Batch 2,480  of  5,541.    Elapsed: 1728.1138896942139.\n",
            "  Batch 2,520  of  5,541.    Elapsed: 1755.9490303993225.\n",
            "  Batch 2,560  of  5,541.    Elapsed: 1783.756592988968.\n",
            "  Batch 2,600  of  5,541.    Elapsed: 1811.5956089496613.\n",
            "  Batch 2,640  of  5,541.    Elapsed: 1839.4136865139008.\n",
            "  Batch 2,680  of  5,541.    Elapsed: 1867.23575258255.\n",
            "  Batch 2,720  of  5,541.    Elapsed: 1895.0589807033539.\n",
            "  Batch 2,760  of  5,541.    Elapsed: 1922.8793442249298.\n",
            "  Batch 2,800  of  5,541.    Elapsed: 1950.6705276966095.\n",
            "  Batch 2,840  of  5,541.    Elapsed: 1978.4756195545197.\n",
            "  Batch 2,880  of  5,541.    Elapsed: 2006.2955210208893.\n",
            "  Batch 2,920  of  5,541.    Elapsed: 2034.1026892662048.\n",
            "  Batch 2,960  of  5,541.    Elapsed: 2061.8948080539703.\n",
            "  Batch 3,000  of  5,541.    Elapsed: 2089.7060630321503.\n",
            "  Batch 3,040  of  5,541.    Elapsed: 2117.498826265335.\n",
            "  Batch 3,080  of  5,541.    Elapsed: 2145.334632396698.\n",
            "  Batch 3,120  of  5,541.    Elapsed: 2173.1329021453857.\n",
            "  Batch 3,160  of  5,541.    Elapsed: 2200.9326033592224.\n",
            "  Batch 3,200  of  5,541.    Elapsed: 2228.7060544490814.\n",
            "  Batch 3,240  of  5,541.    Elapsed: 2256.5247066020966.\n",
            "  Batch 3,280  of  5,541.    Elapsed: 2284.3352036476135.\n",
            "  Batch 3,320  of  5,541.    Elapsed: 2312.120619058609.\n",
            "  Batch 3,360  of  5,541.    Elapsed: 2339.9439165592194.\n",
            "  Batch 3,400  of  5,541.    Elapsed: 2367.801879644394.\n",
            "  Batch 3,440  of  5,541.    Elapsed: 2395.6558549404144.\n",
            "  Batch 3,480  of  5,541.    Elapsed: 2423.4883522987366.\n",
            "  Batch 3,520  of  5,541.    Elapsed: 2451.3438816070557.\n",
            "  Batch 3,560  of  5,541.    Elapsed: 2479.185518503189.\n",
            "  Batch 3,600  of  5,541.    Elapsed: 2507.0170781612396.\n",
            "  Batch 3,640  of  5,541.    Elapsed: 2534.8496878147125.\n",
            "  Batch 3,680  of  5,541.    Elapsed: 2562.700831413269.\n",
            "  Batch 3,720  of  5,541.    Elapsed: 2590.5424885749817.\n",
            "  Batch 3,760  of  5,541.    Elapsed: 2618.3696286678314.\n",
            "  Batch 3,800  of  5,541.    Elapsed: 2646.215363740921.\n",
            "  Batch 3,840  of  5,541.    Elapsed: 2674.053537130356.\n",
            "  Batch 3,880  of  5,541.    Elapsed: 2701.8982961177826.\n",
            "  Batch 3,920  of  5,541.    Elapsed: 2729.739034175873.\n",
            "  Batch 3,960  of  5,541.    Elapsed: 2757.591051340103.\n",
            "  Batch 4,000  of  5,541.    Elapsed: 2785.454419374466.\n",
            "  Batch 4,040  of  5,541.    Elapsed: 2813.3115594387054.\n",
            "  Batch 4,080  of  5,541.    Elapsed: 2841.157923221588.\n",
            "  Batch 4,120  of  5,541.    Elapsed: 2869.00576543808.\n",
            "  Batch 4,160  of  5,541.    Elapsed: 2896.872650384903.\n",
            "  Batch 4,200  of  5,541.    Elapsed: 2924.705394268036.\n",
            "  Batch 4,240  of  5,541.    Elapsed: 2952.5805382728577.\n",
            "  Batch 4,280  of  5,541.    Elapsed: 2980.4468853473663.\n",
            "  Batch 4,320  of  5,541.    Elapsed: 3008.3036983013153.\n",
            "  Batch 4,360  of  5,541.    Elapsed: 3036.1940228939056.\n",
            "  Batch 4,400  of  5,541.    Elapsed: 3064.081964492798.\n",
            "  Batch 4,440  of  5,541.    Elapsed: 3091.9949119091034.\n",
            "  Batch 4,480  of  5,541.    Elapsed: 3119.8657553195953.\n",
            "  Batch 4,520  of  5,541.    Elapsed: 3147.746070623398.\n",
            "  Batch 4,560  of  5,541.    Elapsed: 3175.6136519908905.\n",
            "  Batch 4,600  of  5,541.    Elapsed: 3203.4869678020477.\n",
            "  Batch 4,640  of  5,541.    Elapsed: 3231.371118068695.\n",
            "  Batch 4,680  of  5,541.    Elapsed: 3259.253118991852.\n",
            "  Batch 4,720  of  5,541.    Elapsed: 3287.1383204460144.\n",
            "  Batch 4,760  of  5,541.    Elapsed: 3315.072790622711.\n",
            "  Batch 4,800  of  5,541.    Elapsed: 3342.9660856723785.\n",
            "  Batch 4,840  of  5,541.    Elapsed: 3370.8762152194977.\n",
            "  Batch 4,880  of  5,541.    Elapsed: 3398.7794604301453.\n",
            "  Batch 4,920  of  5,541.    Elapsed: 3426.686201572418.\n",
            "  Batch 4,960  of  5,541.    Elapsed: 3454.5902717113495.\n",
            "  Batch 5,000  of  5,541.    Elapsed: 3482.471086502075.\n",
            "  Batch 5,040  of  5,541.    Elapsed: 3510.3410630226135.\n",
            "  Batch 5,080  of  5,541.    Elapsed: 3538.214637517929.\n",
            "  Batch 5,120  of  5,541.    Elapsed: 3566.0931735038757.\n",
            "  Batch 5,160  of  5,541.    Elapsed: 3593.9812235832214.\n",
            "  Batch 5,200  of  5,541.    Elapsed: 3621.84743642807.\n",
            "  Batch 5,240  of  5,541.    Elapsed: 3649.7010889053345.\n",
            "  Batch 5,280  of  5,541.    Elapsed: 3677.567130804062.\n",
            "  Batch 5,320  of  5,541.    Elapsed: 3705.426548719406.\n",
            "  Batch 5,360  of  5,541.    Elapsed: 3733.2778146266937.\n",
            "  Batch 5,400  of  5,541.    Elapsed: 3761.129989385605.\n",
            "  Batch 5,440  of  5,541.    Elapsed: 3789.0179953575134.\n",
            "  Batch 5,480  of  5,541.    Elapsed: 3816.8851249217987.\n",
            "  Batch 5,520  of  5,541.    Elapsed: 3844.7761557102203.\n",
            "training time for epoch:  0 :  3858.8281767368317\n",
            "Average loss after epoch  0 :  1.3030082\n",
            "  Batch    40  of  5,541.    Elapsed: 3889.7798566818237.\n",
            "  Batch    80  of  5,541.    Elapsed: 3917.668042898178.\n",
            "  Batch   120  of  5,541.    Elapsed: 3945.5414378643036.\n",
            "  Batch   160  of  5,541.    Elapsed: 3973.429411649704.\n",
            "  Batch   200  of  5,541.    Elapsed: 4001.3249232769012.\n",
            "  Batch   240  of  5,541.    Elapsed: 4029.207110643387.\n",
            "  Batch   280  of  5,541.    Elapsed: 4057.093849182129.\n",
            "  Batch   320  of  5,541.    Elapsed: 4084.964951992035.\n",
            "  Batch   360  of  5,541.    Elapsed: 4112.868003129959.\n",
            "  Batch   400  of  5,541.    Elapsed: 4140.735260725021.\n",
            "  Batch   440  of  5,541.    Elapsed: 4168.607427358627.\n",
            "  Batch   480  of  5,541.    Elapsed: 4196.508236885071.\n",
            "  Batch   520  of  5,541.    Elapsed: 4224.358793258667.\n",
            "  Batch   560  of  5,541.    Elapsed: 4252.227061986923.\n",
            "  Batch   600  of  5,541.    Elapsed: 4280.097004413605.\n",
            "  Batch   640  of  5,541.    Elapsed: 4307.938614606857.\n",
            "  Batch   680  of  5,541.    Elapsed: 4335.782908201218.\n",
            "  Batch   720  of  5,541.    Elapsed: 4363.642212629318.\n",
            "  Batch   760  of  5,541.    Elapsed: 4391.496721744537.\n",
            "  Batch   800  of  5,541.    Elapsed: 4419.382301092148.\n",
            "  Batch   840  of  5,541.    Elapsed: 4447.2381756305695.\n",
            "  Batch   880  of  5,541.    Elapsed: 4475.087245941162.\n",
            "  Batch   920  of  5,541.    Elapsed: 4502.926220178604.\n",
            "  Batch   960  of  5,541.    Elapsed: 4530.755810260773.\n",
            "  Batch 1,000  of  5,541.    Elapsed: 4558.589743614197.\n",
            "  Batch 1,040  of  5,541.    Elapsed: 4586.436061620712.\n",
            "  Batch 1,080  of  5,541.    Elapsed: 4614.24729180336.\n",
            "  Batch 1,120  of  5,541.    Elapsed: 4642.061657905579.\n",
            "  Batch 1,160  of  5,541.    Elapsed: 4669.885428905487.\n",
            "  Batch 1,200  of  5,541.    Elapsed: 4697.71374130249.\n",
            "  Batch 1,240  of  5,541.    Elapsed: 4725.514866352081.\n",
            "  Batch 1,280  of  5,541.    Elapsed: 4753.311281204224.\n",
            "  Batch 1,320  of  5,541.    Elapsed: 4781.130084276199.\n",
            "  Batch 1,360  of  5,541.    Elapsed: 4808.938424825668.\n",
            "  Batch 1,400  of  5,541.    Elapsed: 4836.7610738277435.\n",
            "  Batch 1,440  of  5,541.    Elapsed: 4864.59530711174.\n",
            "  Batch 1,480  of  5,541.    Elapsed: 4892.396653652191.\n",
            "  Batch 1,520  of  5,541.    Elapsed: 4920.228548049927.\n",
            "  Batch 1,560  of  5,541.    Elapsed: 4948.040234565735.\n",
            "  Batch 1,600  of  5,541.    Elapsed: 4975.867228984833.\n",
            "  Batch 1,640  of  5,541.    Elapsed: 5003.693454504013.\n",
            "  Batch 1,680  of  5,541.    Elapsed: 5031.534867525101.\n",
            "  Batch 1,720  of  5,541.    Elapsed: 5059.370409011841.\n",
            "  Batch 1,760  of  5,541.    Elapsed: 5087.177955627441.\n",
            "  Batch 1,800  of  5,541.    Elapsed: 5115.034446954727.\n",
            "  Batch 1,840  of  5,541.    Elapsed: 5142.864857435226.\n",
            "  Batch 1,880  of  5,541.    Elapsed: 5170.686643600464.\n",
            "  Batch 1,920  of  5,541.    Elapsed: 5198.546241760254.\n",
            "  Batch 1,960  of  5,541.    Elapsed: 5226.355860233307.\n",
            "  Batch 2,000  of  5,541.    Elapsed: 5254.172534942627.\n",
            "  Batch 2,040  of  5,541.    Elapsed: 5281.990035772324.\n",
            "  Batch 2,080  of  5,541.    Elapsed: 5309.816846132278.\n",
            "  Batch 2,120  of  5,541.    Elapsed: 5337.649946689606.\n",
            "  Batch 2,160  of  5,541.    Elapsed: 5365.458833456039.\n",
            "  Batch 2,200  of  5,541.    Elapsed: 5393.250960111618.\n",
            "  Batch 2,240  of  5,541.    Elapsed: 5421.064088344574.\n",
            "  Batch 2,280  of  5,541.    Elapsed: 5448.887692689896.\n",
            "  Batch 2,320  of  5,541.    Elapsed: 5476.7062702178955.\n",
            "  Batch 2,360  of  5,541.    Elapsed: 5504.522101402283.\n",
            "  Batch 2,400  of  5,541.    Elapsed: 5532.335261583328.\n",
            "  Batch 2,440  of  5,541.    Elapsed: 5560.129064321518.\n",
            "  Batch 2,480  of  5,541.    Elapsed: 5587.984652757645.\n",
            "  Batch 2,520  of  5,541.    Elapsed: 5615.819687128067.\n",
            "  Batch 2,560  of  5,541.    Elapsed: 5643.63668179512.\n",
            "  Batch 2,600  of  5,541.    Elapsed: 5671.456114530563.\n",
            "  Batch 2,640  of  5,541.    Elapsed: 5699.26270198822.\n",
            "  Batch 2,680  of  5,541.    Elapsed: 5727.09402346611.\n",
            "  Batch 2,720  of  5,541.    Elapsed: 5754.948833942413.\n",
            "  Batch 2,760  of  5,541.    Elapsed: 5782.772868871689.\n",
            "  Batch 2,800  of  5,541.    Elapsed: 5810.635633945465.\n",
            "  Batch 2,840  of  5,541.    Elapsed: 5838.4267246723175.\n",
            "  Batch 2,880  of  5,541.    Elapsed: 5866.244024991989.\n",
            "  Batch 2,920  of  5,541.    Elapsed: 5894.07842874527.\n",
            "  Batch 2,960  of  5,541.    Elapsed: 5921.920724630356.\n",
            "  Batch 3,000  of  5,541.    Elapsed: 5949.785692691803.\n",
            "  Batch 3,040  of  5,541.    Elapsed: 5977.6167051792145.\n",
            "  Batch 3,080  of  5,541.    Elapsed: 6005.498873710632.\n",
            "  Batch 3,120  of  5,541.    Elapsed: 6033.334450483322.\n",
            "  Batch 3,160  of  5,541.    Elapsed: 6061.161704301834.\n",
            "  Batch 3,200  of  5,541.    Elapsed: 6088.995574235916.\n",
            "  Batch 3,240  of  5,541.    Elapsed: 6116.862623214722.\n",
            "  Batch 3,280  of  5,541.    Elapsed: 6144.6773381233215.\n",
            "  Batch 3,320  of  5,541.    Elapsed: 6172.49041724205.\n",
            "  Batch 3,360  of  5,541.    Elapsed: 6200.297655820847.\n",
            "  Batch 3,400  of  5,541.    Elapsed: 6228.1313536167145.\n",
            "  Batch 3,440  of  5,541.    Elapsed: 6255.966886043549.\n",
            "  Batch 3,480  of  5,541.    Elapsed: 6283.786330461502.\n",
            "  Batch 3,520  of  5,541.    Elapsed: 6311.641605377197.\n",
            "  Batch 3,560  of  5,541.    Elapsed: 6339.470952749252.\n",
            "  Batch 3,600  of  5,541.    Elapsed: 6367.294624328613.\n",
            "  Batch 3,640  of  5,541.    Elapsed: 6395.144191026688.\n",
            "  Batch 3,680  of  5,541.    Elapsed: 6422.994391441345.\n",
            "  Batch 3,720  of  5,541.    Elapsed: 6450.816914558411.\n",
            "  Batch 3,760  of  5,541.    Elapsed: 6478.659948587418.\n",
            "  Batch 3,800  of  5,541.    Elapsed: 6506.515192270279.\n",
            "  Batch 3,840  of  5,541.    Elapsed: 6534.341034412384.\n",
            "  Batch 3,880  of  5,541.    Elapsed: 6562.204279184341.\n",
            "  Batch 3,920  of  5,541.    Elapsed: 6590.043734550476.\n",
            "  Batch 3,960  of  5,541.    Elapsed: 6617.8900644779205.\n",
            "  Batch 4,000  of  5,541.    Elapsed: 6645.741530895233.\n",
            "  Batch 4,040  of  5,541.    Elapsed: 6673.569977045059.\n",
            "  Batch 4,080  of  5,541.    Elapsed: 6701.406096696854.\n",
            "  Batch 4,120  of  5,541.    Elapsed: 6729.2167110443115.\n",
            "  Batch 4,160  of  5,541.    Elapsed: 6757.036110877991.\n",
            "  Batch 4,200  of  5,541.    Elapsed: 6784.875981330872.\n",
            "  Batch 4,240  of  5,541.    Elapsed: 6812.695659637451.\n",
            "  Batch 4,280  of  5,541.    Elapsed: 6840.514931440353.\n",
            "  Batch 4,320  of  5,541.    Elapsed: 6868.344626903534.\n",
            "  Batch 4,360  of  5,541.    Elapsed: 6896.165883541107.\n",
            "  Batch 4,400  of  5,541.    Elapsed: 6924.0120594501495.\n",
            "  Batch 4,440  of  5,541.    Elapsed: 6951.845074415207.\n",
            "  Batch 4,480  of  5,541.    Elapsed: 6979.688502788544.\n",
            "  Batch 4,520  of  5,541.    Elapsed: 7007.553631067276.\n",
            "  Batch 4,560  of  5,541.    Elapsed: 7035.414041519165.\n",
            "  Batch 4,600  of  5,541.    Elapsed: 7063.27961730957.\n",
            "  Batch 4,640  of  5,541.    Elapsed: 7091.121071577072.\n",
            "  Batch 4,680  of  5,541.    Elapsed: 7118.99396109581.\n",
            "  Batch 4,720  of  5,541.    Elapsed: 7146.799884080887.\n",
            "  Batch 4,760  of  5,541.    Elapsed: 7174.647335529327.\n",
            "  Batch 4,800  of  5,541.    Elapsed: 7202.486440181732.\n",
            "  Batch 4,840  of  5,541.    Elapsed: 7230.324149131775.\n",
            "  Batch 4,880  of  5,541.    Elapsed: 7258.14540886879.\n",
            "  Batch 4,920  of  5,541.    Elapsed: 7285.959657907486.\n",
            "  Batch 4,960  of  5,541.    Elapsed: 7313.771785020828.\n",
            "  Batch 5,000  of  5,541.    Elapsed: 7341.591181278229.\n",
            "  Batch 5,040  of  5,541.    Elapsed: 7369.420323848724.\n",
            "  Batch 5,080  of  5,541.    Elapsed: 7397.267331361771.\n",
            "  Batch 5,120  of  5,541.    Elapsed: 7425.122853040695.\n",
            "  Batch 5,160  of  5,541.    Elapsed: 7452.987374544144.\n",
            "  Batch 5,200  of  5,541.    Elapsed: 7480.848980665207.\n",
            "  Batch 5,240  of  5,541.    Elapsed: 7508.715586423874.\n",
            "  Batch 5,280  of  5,541.    Elapsed: 7536.575075387955.\n",
            "  Batch 5,320  of  5,541.    Elapsed: 7564.426950454712.\n",
            "  Batch 5,360  of  5,541.    Elapsed: 7592.296860218048.\n",
            "  Batch 5,400  of  5,541.    Elapsed: 7620.1810076236725.\n",
            "  Batch 5,440  of  5,541.    Elapsed: 7648.055037736893.\n",
            "  Batch 5,480  of  5,541.    Elapsed: 7675.938796758652.\n",
            "  Batch 5,520  of  5,541.    Elapsed: 7703.790405750275.\n",
            "training time for epoch:  1 :  7717.832026004791\n",
            "Average loss after epoch  1 :  0.8666807\n",
            "  Batch    40  of  5,541.    Elapsed: 7748.354943037033.\n",
            "  Batch    80  of  5,541.    Elapsed: 7776.236243486404.\n",
            "  Batch   120  of  5,541.    Elapsed: 7804.100768089294.\n",
            "  Batch   160  of  5,541.    Elapsed: 7831.987872362137.\n",
            "  Batch   200  of  5,541.    Elapsed: 7859.805087804794.\n",
            "  Batch   240  of  5,541.    Elapsed: 7887.644107341766.\n",
            "  Batch   280  of  5,541.    Elapsed: 7915.475934267044.\n",
            "  Batch   320  of  5,541.    Elapsed: 7943.322266340256.\n",
            "  Batch   360  of  5,541.    Elapsed: 7971.147698640823.\n",
            "  Batch   400  of  5,541.    Elapsed: 7999.005819320679.\n",
            "  Batch   440  of  5,541.    Elapsed: 8026.856503248215.\n",
            "  Batch   480  of  5,541.    Elapsed: 8054.69987988472.\n",
            "  Batch   520  of  5,541.    Elapsed: 8082.548638343811.\n",
            "  Batch   560  of  5,541.    Elapsed: 8110.430554866791.\n",
            "  Batch   600  of  5,541.    Elapsed: 8138.315096139908.\n",
            "  Batch   640  of  5,541.    Elapsed: 8166.177039146423.\n",
            "  Batch   680  of  5,541.    Elapsed: 8194.0629799366.\n",
            "  Batch   720  of  5,541.    Elapsed: 8221.919182777405.\n",
            "  Batch   760  of  5,541.    Elapsed: 8249.785901069641.\n",
            "  Batch   800  of  5,541.    Elapsed: 8277.632603883743.\n",
            "  Batch   840  of  5,541.    Elapsed: 8305.497254610062.\n",
            "  Batch   880  of  5,541.    Elapsed: 8333.344796180725.\n",
            "  Batch   920  of  5,541.    Elapsed: 8361.217343091965.\n",
            "  Batch   960  of  5,541.    Elapsed: 8389.087119340897.\n",
            "  Batch 1,000  of  5,541.    Elapsed: 8416.949407815933.\n",
            "  Batch 1,040  of  5,541.    Elapsed: 8444.783702135086.\n",
            "  Batch 1,080  of  5,541.    Elapsed: 8472.618972063065.\n",
            "  Batch 1,120  of  5,541.    Elapsed: 8500.447820663452.\n",
            "  Batch 1,160  of  5,541.    Elapsed: 8528.298011541367.\n",
            "  Batch 1,200  of  5,541.    Elapsed: 8556.137443304062.\n",
            "  Batch 1,240  of  5,541.    Elapsed: 8583.957503318787.\n",
            "  Batch 1,280  of  5,541.    Elapsed: 8611.803054094315.\n",
            "  Batch 1,320  of  5,541.    Elapsed: 8639.618384361267.\n",
            "  Batch 1,360  of  5,541.    Elapsed: 8667.464643001556.\n",
            "  Batch 1,400  of  5,541.    Elapsed: 8695.2783806324.\n",
            "  Batch 1,440  of  5,541.    Elapsed: 8723.114149332047.\n",
            "  Batch 1,480  of  5,541.    Elapsed: 8750.98556137085.\n",
            "  Batch 1,520  of  5,541.    Elapsed: 8778.834550380707.\n",
            "  Batch 1,560  of  5,541.    Elapsed: 8806.690172433853.\n",
            "  Batch 1,600  of  5,541.    Elapsed: 8834.53620505333.\n",
            "  Batch 1,640  of  5,541.    Elapsed: 8862.390975475311.\n",
            "  Batch 1,680  of  5,541.    Elapsed: 8890.22959446907.\n",
            "  Batch 1,720  of  5,541.    Elapsed: 8918.100410223007.\n",
            "  Batch 1,760  of  5,541.    Elapsed: 8945.95246052742.\n",
            "  Batch 1,800  of  5,541.    Elapsed: 8973.804552555084.\n",
            "  Batch 1,840  of  5,541.    Elapsed: 9001.671530485153.\n",
            "  Batch 1,880  of  5,541.    Elapsed: 9029.516566038132.\n",
            "  Batch 1,920  of  5,541.    Elapsed: 9057.368287801743.\n",
            "  Batch 1,960  of  5,541.    Elapsed: 9085.22898888588.\n",
            "  Batch 2,000  of  5,541.    Elapsed: 9113.075254440308.\n",
            "  Batch 2,040  of  5,541.    Elapsed: 9140.922785758972.\n",
            "  Batch 2,080  of  5,541.    Elapsed: 9168.784720182419.\n",
            "  Batch 2,120  of  5,541.    Elapsed: 9196.659391641617.\n",
            "  Batch 2,160  of  5,541.    Elapsed: 9224.53803563118.\n",
            "  Batch 2,200  of  5,541.    Elapsed: 9252.411842107773.\n",
            "  Batch 2,240  of  5,541.    Elapsed: 9280.265845298767.\n",
            "  Batch 2,280  of  5,541.    Elapsed: 9308.14510846138.\n",
            "  Batch 2,320  of  5,541.    Elapsed: 9336.044486522675.\n",
            "  Batch 2,360  of  5,541.    Elapsed: 9363.92731809616.\n",
            "  Batch 2,400  of  5,541.    Elapsed: 9391.837703943253.\n",
            "  Batch 2,440  of  5,541.    Elapsed: 9419.715250730515.\n",
            "  Batch 2,480  of  5,541.    Elapsed: 9447.602996349335.\n",
            "  Batch 2,520  of  5,541.    Elapsed: 9475.479447126389.\n",
            "  Batch 2,560  of  5,541.    Elapsed: 9503.367023229599.\n",
            "  Batch 2,600  of  5,541.    Elapsed: 9531.260094165802.\n",
            "  Batch 2,640  of  5,541.    Elapsed: 9559.121304273605.\n",
            "  Batch 2,680  of  5,541.    Elapsed: 9587.01274228096.\n",
            "  Batch 2,720  of  5,541.    Elapsed: 9614.878197669983.\n",
            "  Batch 2,760  of  5,541.    Elapsed: 9642.75659584999.\n",
            "  Batch 2,800  of  5,541.    Elapsed: 9670.628711223602.\n",
            "  Batch 2,840  of  5,541.    Elapsed: 9698.510657787323.\n",
            "  Batch 2,880  of  5,541.    Elapsed: 9726.391278982162.\n",
            "  Batch 2,920  of  5,541.    Elapsed: 9754.289845943451.\n",
            "  Batch 2,960  of  5,541.    Elapsed: 9782.14265036583.\n",
            "  Batch 3,000  of  5,541.    Elapsed: 9810.012372016907.\n",
            "  Batch 3,040  of  5,541.    Elapsed: 9837.870642185211.\n",
            "  Batch 3,080  of  5,541.    Elapsed: 9865.746594190598.\n",
            "  Batch 3,120  of  5,541.    Elapsed: 9893.616406917572.\n",
            "  Batch 3,160  of  5,541.    Elapsed: 9921.485050201416.\n",
            "  Batch 3,200  of  5,541.    Elapsed: 9949.354676485062.\n",
            "  Batch 3,240  of  5,541.    Elapsed: 9977.264213323593.\n",
            "  Batch 3,280  of  5,541.    Elapsed: 10005.126833677292.\n",
            "  Batch 3,320  of  5,541.    Elapsed: 10032.984946966171.\n",
            "  Batch 3,360  of  5,541.    Elapsed: 10060.856171369553.\n",
            "  Batch 3,400  of  5,541.    Elapsed: 10088.732162714005.\n",
            "  Batch 3,440  of  5,541.    Elapsed: 10116.61254286766.\n",
            "  Batch 3,480  of  5,541.    Elapsed: 10144.489311695099.\n",
            "  Batch 3,520  of  5,541.    Elapsed: 10172.383886098862.\n",
            "  Batch 3,560  of  5,541.    Elapsed: 10200.265919208527.\n",
            "  Batch 3,600  of  5,541.    Elapsed: 10228.140097856522.\n",
            "  Batch 3,640  of  5,541.    Elapsed: 10256.019701480865.\n",
            "  Batch 3,680  of  5,541.    Elapsed: 10283.94277048111.\n",
            "  Batch 3,720  of  5,541.    Elapsed: 10311.804167747498.\n",
            "  Batch 3,760  of  5,541.    Elapsed: 10339.679012537003.\n",
            "  Batch 3,800  of  5,541.    Elapsed: 10367.565002918243.\n",
            "  Batch 3,840  of  5,541.    Elapsed: 10395.43668103218.\n",
            "  Batch 3,880  of  5,541.    Elapsed: 10423.337196111679.\n",
            "  Batch 3,920  of  5,541.    Elapsed: 10451.205711603165.\n",
            "  Batch 3,960  of  5,541.    Elapsed: 10479.095861196518.\n",
            "  Batch 4,000  of  5,541.    Elapsed: 10506.944766283035.\n",
            "  Batch 4,040  of  5,541.    Elapsed: 10534.812359333038.\n",
            "  Batch 4,080  of  5,541.    Elapsed: 10562.68761253357.\n",
            "  Batch 4,120  of  5,541.    Elapsed: 10590.576631307602.\n",
            "  Batch 4,160  of  5,541.    Elapsed: 10618.4323823452.\n",
            "  Batch 4,200  of  5,541.    Elapsed: 10646.287103652954.\n",
            "  Batch 4,240  of  5,541.    Elapsed: 10674.160072088242.\n",
            "  Batch 4,280  of  5,541.    Elapsed: 10702.020287752151.\n",
            "  Batch 4,320  of  5,541.    Elapsed: 10729.910366535187.\n",
            "  Batch 4,360  of  5,541.    Elapsed: 10757.801763534546.\n",
            "  Batch 4,400  of  5,541.    Elapsed: 10785.702085018158.\n",
            "  Batch 4,440  of  5,541.    Elapsed: 10813.556897163391.\n",
            "  Batch 4,480  of  5,541.    Elapsed: 10841.443506002426.\n",
            "  Batch 4,520  of  5,541.    Elapsed: 10869.328555345535.\n",
            "  Batch 4,560  of  5,541.    Elapsed: 10897.233115434647.\n",
            "  Batch 4,600  of  5,541.    Elapsed: 10925.110353708267.\n",
            "  Batch 4,640  of  5,541.    Elapsed: 10953.022099971771.\n",
            "  Batch 4,680  of  5,541.    Elapsed: 10980.892632722855.\n",
            "  Batch 4,720  of  5,541.    Elapsed: 11008.760949611664.\n",
            "  Batch 4,760  of  5,541.    Elapsed: 11036.618786096573.\n",
            "  Batch 4,800  of  5,541.    Elapsed: 11064.490018844604.\n",
            "  Batch 4,840  of  5,541.    Elapsed: 11092.395567655563.\n",
            "  Batch 4,880  of  5,541.    Elapsed: 11120.245402097702.\n",
            "  Batch 4,920  of  5,541.    Elapsed: 11148.1107442379.\n",
            "  Batch 4,960  of  5,541.    Elapsed: 11175.969043016434.\n",
            "  Batch 5,000  of  5,541.    Elapsed: 11203.838516950607.\n",
            "  Batch 5,040  of  5,541.    Elapsed: 11231.686644077301.\n",
            "  Batch 5,080  of  5,541.    Elapsed: 11259.536976337433.\n",
            "  Batch 5,120  of  5,541.    Elapsed: 11287.396713256836.\n",
            "  Batch 5,160  of  5,541.    Elapsed: 11315.257114887238.\n",
            "  Batch 5,200  of  5,541.    Elapsed: 11343.114229917526.\n",
            "  Batch 5,240  of  5,541.    Elapsed: 11370.98493385315.\n",
            "  Batch 5,280  of  5,541.    Elapsed: 11398.886703968048.\n",
            "  Batch 5,320  of  5,541.    Elapsed: 11426.74938583374.\n",
            "  Batch 5,360  of  5,541.    Elapsed: 11454.620615959167.\n",
            "  Batch 5,400  of  5,541.    Elapsed: 11482.488896131516.\n",
            "  Batch 5,440  of  5,541.    Elapsed: 11510.366324663162.\n",
            "  Batch 5,480  of  5,541.    Elapsed: 11538.24660897255.\n",
            "  Batch 5,520  of  5,541.    Elapsed: 11566.112551927567.\n",
            "training time for epoch:  2 :  11580.152507543564\n",
            "Average loss after epoch  2 :  0.7715269\n",
            "  Batch    40  of  5,541.    Elapsed: 11610.705694437027.\n",
            "  Batch    80  of  5,541.    Elapsed: 11638.562294483185.\n",
            "  Batch   120  of  5,541.    Elapsed: 11666.436744689941.\n",
            "  Batch   160  of  5,541.    Elapsed: 11694.289185285568.\n",
            "  Batch   200  of  5,541.    Elapsed: 11722.14200758934.\n",
            "  Batch   240  of  5,541.    Elapsed: 11749.979449272156.\n",
            "  Batch   280  of  5,541.    Elapsed: 11777.83325624466.\n",
            "  Batch   320  of  5,541.    Elapsed: 11805.711676120758.\n",
            "  Batch   360  of  5,541.    Elapsed: 11833.56939649582.\n",
            "  Batch   400  of  5,541.    Elapsed: 11861.41296172142.\n",
            "  Batch   440  of  5,541.    Elapsed: 11889.279742240906.\n",
            "  Batch   480  of  5,541.    Elapsed: 11917.154886007309.\n",
            "  Batch   520  of  5,541.    Elapsed: 11945.021498680115.\n",
            "  Batch   560  of  5,541.    Elapsed: 11972.894970417023.\n",
            "  Batch   600  of  5,541.    Elapsed: 12000.762097120285.\n",
            "  Batch   640  of  5,541.    Elapsed: 12028.62937951088.\n",
            "  Batch   680  of  5,541.    Elapsed: 12056.495416641235.\n",
            "  Batch   720  of  5,541.    Elapsed: 12084.397855043411.\n",
            "  Batch   760  of  5,541.    Elapsed: 12112.265561580658.\n",
            "  Batch   800  of  5,541.    Elapsed: 12140.134763002396.\n",
            "  Batch   840  of  5,541.    Elapsed: 12167.99194097519.\n",
            "  Batch   880  of  5,541.    Elapsed: 12195.889412879944.\n",
            "  Batch   920  of  5,541.    Elapsed: 12223.755506038666.\n",
            "  Batch   960  of  5,541.    Elapsed: 12251.644718408585.\n",
            "  Batch 1,000  of  5,541.    Elapsed: 12279.495828866959.\n",
            "  Batch 1,040  of  5,541.    Elapsed: 12307.362713336945.\n",
            "  Batch 1,080  of  5,541.    Elapsed: 12335.24161362648.\n",
            "  Batch 1,120  of  5,541.    Elapsed: 12363.114738225937.\n",
            "  Batch 1,160  of  5,541.    Elapsed: 12390.978673696518.\n",
            "  Batch 1,200  of  5,541.    Elapsed: 12418.855705499649.\n",
            "  Batch 1,240  of  5,541.    Elapsed: 12446.743263721466.\n",
            "  Batch 1,280  of  5,541.    Elapsed: 12474.636154651642.\n",
            "  Batch 1,320  of  5,541.    Elapsed: 12502.50379729271.\n",
            "  Batch 1,360  of  5,541.    Elapsed: 12530.393978834152.\n",
            "  Batch 1,400  of  5,541.    Elapsed: 12558.252649784088.\n",
            "  Batch 1,440  of  5,541.    Elapsed: 12586.126496315002.\n",
            "  Batch 1,480  of  5,541.    Elapsed: 12613.997742414474.\n",
            "  Batch 1,520  of  5,541.    Elapsed: 12641.868458747864.\n",
            "  Batch 1,560  of  5,541.    Elapsed: 12669.70177078247.\n",
            "  Batch 1,600  of  5,541.    Elapsed: 12697.537325143814.\n",
            "  Batch 1,640  of  5,541.    Elapsed: 12725.404832601547.\n",
            "  Batch 1,680  of  5,541.    Elapsed: 12753.301396608353.\n",
            "  Batch 1,720  of  5,541.    Elapsed: 12781.14533662796.\n",
            "  Batch 1,760  of  5,541.    Elapsed: 12809.027163028717.\n",
            "  Batch 1,800  of  5,541.    Elapsed: 12836.88545370102.\n",
            "  Batch 1,840  of  5,541.    Elapsed: 12864.78013253212.\n",
            "  Batch 1,880  of  5,541.    Elapsed: 12892.648258447647.\n",
            "  Batch 1,920  of  5,541.    Elapsed: 12920.550993919373.\n",
            "  Batch 1,960  of  5,541.    Elapsed: 12948.436101198196.\n",
            "  Batch 2,000  of  5,541.    Elapsed: 12976.321679830551.\n",
            "  Batch 2,040  of  5,541.    Elapsed: 13004.189204454422.\n",
            "  Batch 2,080  of  5,541.    Elapsed: 13032.064079999924.\n",
            "  Batch 2,120  of  5,541.    Elapsed: 13059.94770026207.\n",
            "  Batch 2,160  of  5,541.    Elapsed: 13087.812749624252.\n",
            "  Batch 2,200  of  5,541.    Elapsed: 13115.688196897507.\n",
            "  Batch 2,240  of  5,541.    Elapsed: 13143.52797961235.\n",
            "  Batch 2,280  of  5,541.    Elapsed: 13171.427102088928.\n",
            "  Batch 2,320  of  5,541.    Elapsed: 13199.308819532394.\n",
            "  Batch 2,360  of  5,541.    Elapsed: 13227.189118862152.\n",
            "  Batch 2,400  of  5,541.    Elapsed: 13255.066002130508.\n",
            "  Batch 2,440  of  5,541.    Elapsed: 13282.949267625809.\n",
            "  Batch 2,480  of  5,541.    Elapsed: 13310.824598550797.\n",
            "  Batch 2,520  of  5,541.    Elapsed: 13338.739006280899.\n",
            "  Batch 2,560  of  5,541.    Elapsed: 13366.625824928284.\n",
            "  Batch 2,600  of  5,541.    Elapsed: 13394.521688461304.\n",
            "  Batch 2,640  of  5,541.    Elapsed: 13422.409884929657.\n",
            "  Batch 2,680  of  5,541.    Elapsed: 13450.304411411285.\n",
            "  Batch 2,720  of  5,541.    Elapsed: 13478.227224111557.\n",
            "  Batch 2,760  of  5,541.    Elapsed: 13506.12655711174.\n",
            "  Batch 2,800  of  5,541.    Elapsed: 13533.999303340912.\n",
            "  Batch 2,840  of  5,541.    Elapsed: 13561.883741140366.\n",
            "  Batch 2,880  of  5,541.    Elapsed: 13589.773436784744.\n",
            "  Batch 2,920  of  5,541.    Elapsed: 13617.663568973541.\n",
            "  Batch 2,960  of  5,541.    Elapsed: 13645.546591043472.\n",
            "  Batch 3,000  of  5,541.    Elapsed: 13673.436835050583.\n",
            "  Batch 3,040  of  5,541.    Elapsed: 13701.299275636673.\n",
            "  Batch 3,080  of  5,541.    Elapsed: 13729.1495616436.\n",
            "  Batch 3,120  of  5,541.    Elapsed: 13756.977152585983.\n",
            "  Batch 3,160  of  5,541.    Elapsed: 13784.84332561493.\n",
            "  Batch 3,200  of  5,541.    Elapsed: 13812.716330051422.\n",
            "  Batch 3,240  of  5,541.    Elapsed: 13840.611781358719.\n",
            "  Batch 3,280  of  5,541.    Elapsed: 13868.503409385681.\n",
            "  Batch 3,320  of  5,541.    Elapsed: 13896.421139478683.\n",
            "  Batch 3,360  of  5,541.    Elapsed: 13924.331141233444.\n",
            "  Batch 3,400  of  5,541.    Elapsed: 13952.24145746231.\n",
            "  Batch 3,440  of  5,541.    Elapsed: 13980.110438108444.\n",
            "  Batch 3,480  of  5,541.    Elapsed: 14008.025015354156.\n",
            "  Batch 3,520  of  5,541.    Elapsed: 14035.94160437584.\n",
            "  Batch 3,560  of  5,541.    Elapsed: 14063.878006696701.\n",
            "  Batch 3,600  of  5,541.    Elapsed: 14091.78034234047.\n",
            "  Batch 3,640  of  5,541.    Elapsed: 14119.656539916992.\n",
            "  Batch 3,680  of  5,541.    Elapsed: 14147.533313512802.\n",
            "  Batch 3,720  of  5,541.    Elapsed: 14175.436621427536.\n",
            "  Batch 3,760  of  5,541.    Elapsed: 14203.297633171082.\n",
            "  Batch 3,800  of  5,541.    Elapsed: 14231.214163303375.\n",
            "  Batch 3,840  of  5,541.    Elapsed: 14259.081968307495.\n",
            "  Batch 3,880  of  5,541.    Elapsed: 14286.962293863297.\n",
            "  Batch 3,920  of  5,541.    Elapsed: 14314.857921123505.\n",
            "  Batch 3,960  of  5,541.    Elapsed: 14342.731921434402.\n",
            "  Batch 4,000  of  5,541.    Elapsed: 14370.595301151276.\n",
            "  Batch 4,040  of  5,541.    Elapsed: 14398.46308350563.\n",
            "  Batch 4,080  of  5,541.    Elapsed: 14426.346368551254.\n",
            "  Batch 4,120  of  5,541.    Elapsed: 14454.22284078598.\n",
            "  Batch 4,160  of  5,541.    Elapsed: 14482.09150481224.\n",
            "  Batch 4,200  of  5,541.    Elapsed: 14509.979420423508.\n",
            "  Batch 4,240  of  5,541.    Elapsed: 14537.832991361618.\n",
            "  Batch 4,280  of  5,541.    Elapsed: 14565.730814933777.\n",
            "  Batch 4,320  of  5,541.    Elapsed: 14593.594867944717.\n",
            "  Batch 4,360  of  5,541.    Elapsed: 14621.504511833191.\n",
            "  Batch 4,400  of  5,541.    Elapsed: 14649.380276441574.\n",
            "  Batch 4,440  of  5,541.    Elapsed: 14677.259008407593.\n",
            "  Batch 4,480  of  5,541.    Elapsed: 14705.123500347137.\n",
            "  Batch 4,520  of  5,541.    Elapsed: 14733.014004945755.\n",
            "  Batch 4,560  of  5,541.    Elapsed: 14760.887674093246.\n",
            "  Batch 4,600  of  5,541.    Elapsed: 14788.805818796158.\n",
            "  Batch 4,640  of  5,541.    Elapsed: 14816.706749200821.\n",
            "  Batch 4,680  of  5,541.    Elapsed: 14844.613388061523.\n",
            "  Batch 4,720  of  5,541.    Elapsed: 14872.52643275261.\n",
            "  Batch 4,760  of  5,541.    Elapsed: 14900.417984008789.\n",
            "  Batch 4,800  of  5,541.    Elapsed: 14928.31305861473.\n",
            "  Batch 4,840  of  5,541.    Elapsed: 14956.207371473312.\n",
            "  Batch 4,880  of  5,541.    Elapsed: 14984.121663093567.\n",
            "  Batch 4,920  of  5,541.    Elapsed: 15011.99616575241.\n",
            "  Batch 4,960  of  5,541.    Elapsed: 15039.896793365479.\n",
            "  Batch 5,000  of  5,541.    Elapsed: 15067.762514829636.\n",
            "  Batch 5,040  of  5,541.    Elapsed: 15095.628670215607.\n",
            "  Batch 5,080  of  5,541.    Elapsed: 15123.533586502075.\n",
            "  Batch 5,120  of  5,541.    Elapsed: 15151.39730000496.\n",
            "  Batch 5,160  of  5,541.    Elapsed: 15179.270136833191.\n",
            "  Batch 5,200  of  5,541.    Elapsed: 15207.128121137619.\n",
            "  Batch 5,240  of  5,541.    Elapsed: 15235.009687423706.\n",
            "  Batch 5,280  of  5,541.    Elapsed: 15262.909574985504.\n",
            "  Batch 5,320  of  5,541.    Elapsed: 15290.779619455338.\n",
            "  Batch 5,360  of  5,541.    Elapsed: 15318.689632177353.\n",
            "  Batch 5,400  of  5,541.    Elapsed: 15346.5551943779.\n",
            "  Batch 5,440  of  5,541.    Elapsed: 15374.45054268837.\n",
            "  Batch 5,480  of  5,541.    Elapsed: 15402.307314395905.\n",
            "  Batch 5,520  of  5,541.    Elapsed: 15430.212680578232.\n",
            "training time for epoch:  3 :  15444.282945394516\n",
            "Average loss after epoch  3 :  0.7708566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMW0lX06Z2fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the trained model (Create the directory if it does not exist; otherwise override the contents)\n",
        "import os\n",
        "if not os.path.exists('./saved_model'):\n",
        "    os.makedirs('./saved_model')\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained('./saved_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kmGxaB0YJz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "c1fcf556-495d-4e80-c74c-10d3100d99a1"
      },
      "source": [
        "#Validation of the model on dev dataset \n",
        "\n",
        "print(\"Validating the model...\")\n",
        "model.eval()\n",
        "results = []\n",
        "for input_ids, input_masks, segment_ids, example_indices in dev_dataloader:\n",
        "  input_ids = input_ids.to(device)\n",
        "  input_masks = input_masks.to(device)    \n",
        "  segment_ids = segment_ids.to(device)    \n",
        "  with torch.no_grad(): \n",
        "    batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_masks)\n",
        "  for i, example_index in enumerate(example_indices):\n",
        "    start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
        "    end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
        "    dev_feature = dev_features[example_index.item()]\n",
        "    unique_id = int(dev_feature.unique_id)\n",
        "    rawResult = RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits)\n",
        "    results.append(rawResult)\n",
        "        \n",
        "if not os.path.exists('./model_eval'):\n",
        "    os.makedirs('./model_eval')\n",
        "output_prediction_file = os.path.join(\"./model_eval\", \"predictions.json\")\n",
        "output_nbest_file = os.path.join(\"/model_eval\", \"nbest_predictions.json\")\n",
        "output_null_log_odds_file = os.path.join(\"./model_eval\", \"null_odds.json\")\n",
        "\n",
        "preds = write_predictions(dev_examples, dev_features, results, 20, 30, True, output_prediction_file, output_nbest_file, output_null_log_odds_file, True, False, 0.0)     "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validating the model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-17c61f97665b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moutput_null_log_odds_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model_eval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"null_odds.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_prediction_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_nbest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_null_log_odds_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/utils_squad.py\u001b[0m in \u001b[0;36mwrite_predictions\u001b[0;34m(all_examples, all_features, all_results, n_best_size, max_answer_length, do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, verbose_logging, version_2_with_negative, null_score_diff_threshold)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_nbest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_nbest_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/model_eval/nbest_predictions.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0siF0yBrGng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fe0d74d-144b-4d1a-f29b-b754215d085a"
      },
      "source": [
        "%run evaluate-v1.1.py dev-v1.1.json ./model_eval/predictions.json"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"exact_match\": 79.81078524124882, \"f1\": 87.50959699791207}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}