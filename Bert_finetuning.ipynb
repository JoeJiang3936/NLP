{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoeJiang3936/NLP/blob/master/Bert_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npmppsaFWFtb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "40ea0e2c-1638-4b4d-cf1a-ed183abe929f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.19)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGXeTmDEPSsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCkbL-SFPTZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6e7bfe6-057c-4bdc-9f59-c320a9753562"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjqiadnZPTqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvpkpNtmPUSk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24ebbb54-68b7-4acb-bb22-970812b95aa1"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
        "  print(\"We will use GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print('There are no GPU available, using the CPU instead.')\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_j6KWbZPUX0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3a68c5f8-3b3d-4b27-9569-2d33219eaae7"
      },
      "source": [
        "#CoLA dataset, a small dataset containing 10,000 sentences. The task: whether a sentence is grammatically correct or not. \n",
        "!pip install wget\n",
        "import wget\n",
        "import os\n",
        "print(\"Downloading the CoLA dataset ...\")\n",
        "\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Downloading the CoLA dataset ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./cola_public_1.1 (1).zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kN_sMwES0-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('./cola_public/'):\n",
        "  !unzip cola_public_1.1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKNx9fGwPUaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "122330a1-fb71-405a-d062-39ed7682c3b1"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter = '\\t', header = None, names = [\"sentence_source\", \"label\", \"label_notes\", \"sentence\"])\n",
        "print(\"The number of sentences :\", df.shape[0])\n",
        "df.head(10)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of sentences : 8551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our friends won't buy this analysis, let alone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One more pseudo generalization and I'm giving up.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One more pseudo generalization or I'm giving up.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The more we study verbs, the crazier they get.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Day by day the facts are getting murkier.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'll fix you a drink.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fred watered the plants flat.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bill coughed his way out of the restaurant.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We're dancing the night away.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gj04</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Herman hammered the metal flat.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentence_source  ...                                           sentence\n",
              "0            gj04  ...  Our friends won't buy this analysis, let alone...\n",
              "1            gj04  ...  One more pseudo generalization and I'm giving up.\n",
              "2            gj04  ...   One more pseudo generalization or I'm giving up.\n",
              "3            gj04  ...     The more we study verbs, the crazier they get.\n",
              "4            gj04  ...          Day by day the facts are getting murkier.\n",
              "5            gj04  ...                              I'll fix you a drink.\n",
              "6            gj04  ...                      Fred watered the plants flat.\n",
              "7            gj04  ...        Bill coughed his way out of the restaurant.\n",
              "8            gj04  ...                      We're dancing the night away.\n",
              "9            gj04  ...                    Herman hammered the metal flat.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbAKRY7IPUc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "3f33ac4e-2b13-4e04-ca4d-c7650f6d26cd"
      },
      "source": [
        "df.loc[df.label == 0].sample(10)[['sentence','label']]\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5465</th>\n",
              "      <td>I have six of them more.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2105</th>\n",
              "      <td>I lent Tony the book halfway.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>the bottle drained the liquid free.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3188</th>\n",
              "      <td>She always clad herself in black.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3477</th>\n",
              "      <td>the box contains little tool.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>They tried all to like John.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7365</th>\n",
              "      <td>It mattered with a telescope.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1823</th>\n",
              "      <td>It started to rain after Jackie and me, we had...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3926</th>\n",
              "      <td>Sketch by his students appeared in the magazine.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6751</th>\n",
              "      <td>Because did Marianne love Willoughby, she refu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "5465                           I have six of them more.      0\n",
              "2105                      I lent Tony the book halfway.      0\n",
              "591                 the bottle drained the liquid free.      0\n",
              "3188                  She always clad herself in black.      0\n",
              "3477                      the box contains little tool.      0\n",
              "496                        They tried all to like John.      0\n",
              "7365                      It mattered with a telescope.      0\n",
              "1823  It started to rain after Jackie and me, we had...      0\n",
              "3926   Sketch by his students appeared in the magazine.      0\n",
              "6751  Because did Marianne love Willoughby, she refu...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un5RH94rPUff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3ebcadfe-e8c0-4778-f637-93d69b3e3a34"
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "print(sentences[:10], labels[:10]) "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Our friends won't buy this analysis, let alone the next one we propose.\"\n",
            " \"One more pseudo generalization and I'm giving up.\"\n",
            " \"One more pseudo generalization or I'm giving up.\"\n",
            " 'The more we study verbs, the crazier they get.'\n",
            " 'Day by day the facts are getting murkier.' \"I'll fix you a drink.\"\n",
            " 'Fred watered the plants flat.'\n",
            " 'Bill coughed his way out of the restaurant.'\n",
            " \"We're dancing the night away.\" 'Herman hammered the metal flat.'] [1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUSEOLYKPUh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f97d34d-2242-401e-8369-07da3002cfd4"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "print('Load BERT tokenizer ...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load BERT tokenizer ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO6QCGuGPUkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5dc4b26d-44cb-4eae-df26-7ffa973b80f2"
      },
      "source": [
        "print(\"Original sentence: \", sentences[0])\n",
        "print('Tokenized sentence: ', tokenizer.tokenize(sentences[0]))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized sentence:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR1IZEwgPUmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "168f2592-8783-4f9c-c7cc-ed25b38478da"
      },
      "source": [
        "#convert into IDs\n",
        "input_ids = []\n",
        "for sentence in sentences:\n",
        "  encoded_sentence = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "  input_ids.append(encoded_sentence)\n",
        "\n",
        "print('Original sentence: ', sentences[1])\n",
        "print('Encoded sentence IDs: ', input_ids[1])\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence:  One more pseudo generalization and I'm giving up.\n",
            "Encoded sentence IDs:  [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcwXXe1aPUpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "65db694e-84e3-4db5-ff76-1a79ed9b5091"
      },
      "source": [
        "#padding and truncating\n",
        "max_lens = max(len(sentence) for sentence in input_ids)\n",
        "print('The longest sentence in the dataset: ', max_lens)\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN  = 48\n",
        "print('padding all sentences to MAX_LEN = %d...'%MAX_LEN)\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', value=0, truncating='post', padding='post')\n",
        "input_ids[1]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The longest sentence in the dataset:  47\n",
            "padding all sentences to MAX_LEN = 48...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  101,  2028,  2062, 18404,  2236,  3989,  1998,  1045,  1005,\n",
              "        1049,  3228,  2039,  1012,   102,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-97nIKrcz_LT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "fe2be68b-5892-4191-a72b-2a5ad571fdb7"
      },
      "source": [
        "#add attention masks\n",
        "attention_masks = []\n",
        "for sentence in input_ids:\n",
        "  mask = [int(id>0) for id in sentence]\n",
        "  attention_masks.append(mask)\n",
        "attention_masks[0]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAM7HRbmz_OF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "aaea3c00-9472-40ec-9a45-7f0a9a2b7078"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state = 2018, test_size = 0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state = 2018, test_size = 0.1)\n",
        "print('training dataset size: ', len(train_inputs))\n",
        "print('validation dataset size: ', len(validation_inputs))\n",
        "print(len(train_masks), len(train_masks[0]))\n",
        "print(len(train_inputs), len(train_inputs[0]))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training dataset size:  7695\n",
            "validation dataset size:  856\n",
            "7695 48\n",
            "7695 48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhHnkYV-z_Qc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bef1dd38-b12b-4b53-e954-d685866c3647"
      },
      "source": [
        "#converting to Pytorch datatypes\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "print(train_inputs.shape)\n",
        "print(train_masks.shape)\n",
        "print(validation_inputs.shape)\n",
        "print(validation_masks.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7695, 48])\n",
            "torch.Size([7695, 48])\n",
            "torch.Size([856, 48])\n",
            "torch.Size([856, 48])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PywY5Ap3z_Sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use dataloader to train the model in batches, it is more memory-efficient than a for loop\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = RandomSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler = validation_sampler, batch_size = batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuR2-hug7ntW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0c235d0-ca43-49b6-95d7-3524e5a1f2bd"
      },
      "source": [
        "#fine tuning the bert model\n",
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
        "model.cuda()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozcU2RUU7nxB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "0117a04c-3000-44f8-86ed-d87501cf5e86"
      },
      "source": [
        "#exploring the weights of the bert model\n",
        "params = list(model.named_parameters())\n",
        "print('The Bert base model has {:} different named parameters. \\n'.format(len(params)))\n",
        "print('Embedding layer')\n",
        "for parameter in params[0:5]:\n",
        "  print(\"{:<55} {:>12}\".format(parameter[0], str(tuple(parameter[1].size()))))\n",
        "print('\\nFirst Transformer')\n",
        "for parameter in params[5:21]:\n",
        "  print(\"{:<55} {:>12}\".format(parameter[0], str(tuple(parameter[1].size()))))\n",
        "print('\\nOutput layer')\n",
        "for parameter in params[-4:]:\n",
        "  print(\"{:<55} {:>12}\".format(parameter[0], str(tuple(parameter[1].size()))))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Bert base model has 201 different named parameters. \n",
            "\n",
            "Embedding layer\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "First Transformer\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "Output layer\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ9J0aie7ny1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the optimizer\n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8, weight_decay = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-EqJB-v7n00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 12\n",
        "total_steps = len(train_dataloader)*epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUrj8v5n7n27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#classification accuracy function\n",
        "def accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAef_hBK7n4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round(elapsed))\n",
        "  return str(datetime.timedelta(seconds = elapsed_rounded)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWk4vJ6p7n8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c78cb70-c3ae-4eef-ca5a-9e49a42ed572"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('training at epoch: ', epoch +1)\n",
        "  t0 = time.time()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    if step%40 == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "\n",
        "      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "    model.train()\n",
        "\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    outputs = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask, labels = b_labels)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    total_loss += loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    model.zero_grad()\n",
        "  \n",
        "  avg_train_loss = total_loss/len(train_dataloader)\n",
        "\n",
        "  loss_values.append(avg_train_loss)\n",
        "\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  print('validating...')\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  eval_loss, eval_accuracy = 0, 0 \n",
        "  nb_eval_steps, nb_eval_examples = 0, 0 \n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  b_input_ids, b_input_mask, b_labels = batch    \n",
        "        \n",
        "  with torch.no_grad():\n",
        "    outputs = model(b_input_ids, token_type_ids = None, attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  tmp_eval_accuracy = accuracy(logits, label_ids)\n",
        "  eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "  nb_eval_steps += 1\n",
        "\n",
        "  print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "  print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training at epoch:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:31.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:46.\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:00:46\n",
            "validating...\n",
            "  Accuracy: 0.75\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  2\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:46.\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:00:46\n",
            "validating...\n",
            "  Accuracy: 0.92\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  3\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:31.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:46.\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:00:46\n",
            "validating...\n",
            "  Accuracy: 0.92\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  4\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:46.\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:00:46\n",
            "validating...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  5\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:46.\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:00:46\n",
            "validating...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  6\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:45.\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:00:46\n",
            "validating...\n",
            "  Accuracy: 0.75\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  7\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:45.\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:00:45\n",
            "validating...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  8\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:45.\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:00:45\n",
            "validating...\n",
            "  Accuracy: 0.92\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  9\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:45.\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:00:45\n",
            "validating...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  10\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:45.\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:45\n",
            "validating...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  11\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:45.\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:45\n",
            "validating...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:00\n",
            "training at epoch:  12\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:38.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:45.\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:45\n",
            "validating...\n",
            "  Accuracy: 0.75\n",
            "  Validation took: 0:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEKF92TE7n-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "d71aea5f-e4e8-4ed3-e053-62a6cd41dcf8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1az6vObA7n_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c371bdf2-d1bd-4ea0-d02f-b576cf8dedb5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the test dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMFv0ZnSz_UP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3892d7dc-bc96-424b-a0d9-babcc8d6a072"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBVuafKxz_XJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae396ea8-251b-43b2-e24d-699a3eb44954"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0bC-1Unz_ph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "58e27486-f905-4357-a42e-884acb5a38bb"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6IamvTOz_sK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b5275a7-e80b-45d9-e380-23680c46ce36"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nNSAMDMz_vW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cix2bDt2z_wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQOd9VB4z_yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}