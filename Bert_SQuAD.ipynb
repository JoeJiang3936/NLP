{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_SQuAD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoeJiang3936/NLP/blob/master/Bert_SQuAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBcMIY_mJZeN",
        "colab_type": "code",
        "outputId": "6946e69c-0f7e-4afa-a307-c9a5ff89011f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 30.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 8.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\r\u001b[K     |▍                               | 10kB 26.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 30.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 36.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 24.2MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 16.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 71kB 15.7MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 16.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 17.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 102kB 17.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 112kB 17.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 122kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 133kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 143kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 153kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 174kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 184kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 194kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 204kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 215kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 225kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 235kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 245kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 256kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 266kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 276kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 286kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 296kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 307kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 317kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 327kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 337kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 348kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 358kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 368kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 378kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 389kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 399kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 409kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 419kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 430kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 440kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 450kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 460kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 471kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 481kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 491kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 501kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 512kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 522kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 532kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 542kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 552kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 563kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 573kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 583kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 593kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 604kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 614kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 624kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 634kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 645kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 655kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 665kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 675kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 686kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 696kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 706kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 716kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 727kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 737kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 747kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 757kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 768kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 778kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 788kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 798kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 808kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 819kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 829kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 839kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 849kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 860kB 17.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.10.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.9)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.17.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.13.40)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=518261d10ad3bfcd4a3b024613228629ac1b2b3a78fb08a444552cb130776d31\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.35 sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v383Ox4GNRcx",
        "colab_type": "code",
        "outputId": "17fa9bda-2540-4ced-870c-58f3e2d3d6ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 33.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 6.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 9.2MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.40)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.40)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (2.6.1)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zohPhxfeUpBV",
        "colab_type": "code",
        "outputId": "c6ce0768-27dd-4339-adf6-d54bb53dc50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=a1d5b36a6faf45a5a277a6337fbade32838727516eb1a3f3d716dbb0e85e34a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho1PLx7uq0l7",
        "colab_type": "code",
        "outputId": "5d95fbc0-5096-4464-91f1-5724e39fc49d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import wget\n",
        "wget.download(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
        "wget.download(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
        "wget.download(\"https://raw.githubusercontent.com/allenai/bi-att-flow/master/squad/evaluate-v1.1.py\")\n",
        "wget.download(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\")\n",
        "wget.download('https://raw.githubusercontent.com/nlpyang/pytorch-transformers/master/examples/utils_squad.py')\n",
        "wget.download('https://raw.githubusercontent.com/nlpyang/pytorch-transformers/master/examples/utils_squad_evaluate.py')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'utils_squad_evaluate.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTN96UpOUvlv",
        "colab_type": "code",
        "outputId": "4138fe39-46c8-4eb5-ec54-389bbbfa0fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuD7r3plDD2B",
        "colab_type": "code",
        "outputId": "380c78db-4bd2-4007-e970-6a9178e74a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
        "  print(\"We will use GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print('There are no GPU available, using the CPU instead.')\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huN8_5KwUmW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from pytorch_transformers import BertForQuestionAnswering, BertTokenizer\n",
        "from utils_squad import read_squad_examples, convert_examples_to_features, RawResult, write_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYxP5pyBQPNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 2\n",
        "batch_size = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ-2qcgmQPPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEHqQlAgh9qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxrg0Nk9tpSE",
        "colab_type": "code",
        "outputId": "d0380b0b-2863-4479-ad5b-f686683476c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#initialize Bert base model \n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 173998.30B/s]\n",
            "100%|██████████| 440473133/440473133 [00:17<00:00, 25414589.38B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmQLB7jfQPRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = './train-v1.1.json'\n",
        "train_examples = read_squad_examples(train_file, is_training = True, version_2_with_negative=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LnGqZY1QPTw",
        "colab_type": "code",
        "outputId": "d5e5b417-3057-4d9a-ea41-9642644076e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_examples)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87599"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV9BlrzJkSVD",
        "colab_type": "code",
        "outputId": "706e6cc8-b8ba-40c9-c16f-a305c25ef320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "train_examples[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "qas_id: 5733be284776f41900661182, question_text: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?, doc_tokens: [Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.], start_position: 90, end_position: 92"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udk4bjXAQPWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenize the training examples\n",
        "tokenizer = BertTokenizer(vocab_file='bert-base-uncased-vocab.txt')\n",
        "train_features = convert_examples_to_features(train_examples, tokenizer, max_seq_length=384, doc_stride=128, max_query_length=64, is_training=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_FaOS7yQPYw",
        "colab_type": "code",
        "outputId": "4496410c-8714-4264-a6e9-8805f6ccc343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_features)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "033mb0-ltokh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the tokenized data into torch tensors\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype = torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype = torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype = torch.long)\n",
        "all_start_positions = torch.tensor([f.start_position for f in train_features], dtype = torch.long)\n",
        "all_end_positions = torch.tensor([f.end_position for f in train_features], dtype = torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIiqXQvBtovf",
        "colab_type": "code",
        "outputId": "92b8ced9-8b75-4276-e470-c8738b14b147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "all_input_ids[1], all_input_mask[1], all_segment_ids[1], all_start_positions[1], all_end_positions[1]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  101,  2054,  2003,  1999,  2392,  1997,  1996, 10289,  8214,  2364,\n",
              "          2311,  1029,   102,  6549,  2135,  1010,  1996,  2082,  2038,  1037,\n",
              "          3234,  2839,  1012, 10234,  1996,  2364,  2311,  1005,  1055,  2751,\n",
              "          8514,  2003,  1037,  3585,  6231,  1997,  1996,  6261,  2984,  1012,\n",
              "          3202,  1999,  2392,  1997,  1996,  2364,  2311,  1998,  5307,  2009,\n",
              "          1010,  2003,  1037,  6967,  6231,  1997,  4828,  2007,  2608,  2039,\n",
              "         14995,  6924,  2007,  1996,  5722,  1000,  2310,  3490,  2618,  4748,\n",
              "          2033, 18168,  5267,  1000,  1012,  2279,  2000,  1996,  2364,  2311,\n",
              "          2003,  1996, 13546,  1997,  1996,  6730,  2540,  1012,  3202,  2369,\n",
              "          1996, 13546,  2003,  1996, 24665, 23052,  1010,  1037, 14042,  2173,\n",
              "          1997,  7083,  1998,  9185,  1012,  2009,  2003,  1037, 15059,  1997,\n",
              "          1996, 24665, 23052,  2012, 10223, 26371,  1010,  2605,  2073,  1996,\n",
              "          6261,  2984, 22353,  2135,  2596,  2000,  3002, 16595,  9648,  4674,\n",
              "          2061, 12083,  9711,  2271,  1999,  8517,  1012,  2012,  1996,  2203,\n",
              "          1997,  1996,  2364,  3298,  1006,  1998,  1999,  1037,  3622,  2240,\n",
              "          2008,  8539,  2083,  1017, 11342,  1998,  1996,  2751,  8514,  1007,\n",
              "          1010,  2003,  1037,  3722,  1010,  2715,  2962,  6231,  1997,  2984,\n",
              "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor(52),\n",
              " tensor(56))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHo2dfjBtpCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_start_positions, all_end_positions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qI4EQCttpK2",
        "colab_type": "code",
        "outputId": "7a22f04f-c935-455b-a5f7-54785489fea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_data[1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  101,  2054,  2003,  1999,  2392,  1997,  1996, 10289,  8214,  2364,\n",
              "          2311,  1029,   102,  6549,  2135,  1010,  1996,  2082,  2038,  1037,\n",
              "          3234,  2839,  1012, 10234,  1996,  2364,  2311,  1005,  1055,  2751,\n",
              "          8514,  2003,  1037,  3585,  6231,  1997,  1996,  6261,  2984,  1012,\n",
              "          3202,  1999,  2392,  1997,  1996,  2364,  2311,  1998,  5307,  2009,\n",
              "          1010,  2003,  1037,  6967,  6231,  1997,  4828,  2007,  2608,  2039,\n",
              "         14995,  6924,  2007,  1996,  5722,  1000,  2310,  3490,  2618,  4748,\n",
              "          2033, 18168,  5267,  1000,  1012,  2279,  2000,  1996,  2364,  2311,\n",
              "          2003,  1996, 13546,  1997,  1996,  6730,  2540,  1012,  3202,  2369,\n",
              "          1996, 13546,  2003,  1996, 24665, 23052,  1010,  1037, 14042,  2173,\n",
              "          1997,  7083,  1998,  9185,  1012,  2009,  2003,  1037, 15059,  1997,\n",
              "          1996, 24665, 23052,  2012, 10223, 26371,  1010,  2605,  2073,  1996,\n",
              "          6261,  2984, 22353,  2135,  2596,  2000,  3002, 16595,  9648,  4674,\n",
              "          2061, 12083,  9711,  2271,  1999,  8517,  1012,  2012,  1996,  2203,\n",
              "          1997,  1996,  2364,  3298,  1006,  1998,  1999,  1037,  3622,  2240,\n",
              "          2008,  8539,  2083,  1017, 11342,  1998,  1996,  2751,  8514,  1007,\n",
              "          1010,  2003,  1037,  3722,  1010,  2715,  2962,  6231,  1997,  2984,\n",
              "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor(52),\n",
              " tensor(56))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PFXcXUztpOl",
        "colab_type": "code",
        "outputId": "294dc85a-056d-4211-8495-6ba9975db56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "len(train_dataloader)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5541"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhjK9ihztpVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set up the optimizer\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "optimizer = AdamW(model.parameters(), lr = 3e-5, eps = 1e-4, correct_bias=False)\n",
        "total_steps = len(train_dataloader)*epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8N2I7H7yT-7",
        "colab_type": "code",
        "outputId": "62fbc6df-624f-480c-c473-10f73d069a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "import time \n",
        "\n",
        "print('Training the model ...')\n",
        "time0 = time.time()\n",
        "lossList = []\n",
        "for epoch in range(4):  \n",
        "  total_loss = 0\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    model.train()\n",
        "    if step%40 == 0 and not step == 0:\n",
        "      elapsed = time.time() - time0\n",
        "      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "   \n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
        "    outputs = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
        "    loss = outputs[0]\n",
        "    total_loss += loss\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    model.zero_grad()\n",
        "  avg_train_loss = (total_loss/len(train_dataloader)).detach().cpu().numpy()\n",
        "  lossList.append(avg_train_loss)\n",
        "  print('training time for epoch: ', epoch, ': ', time.time()-time0)\n",
        "  print('Average loss after epoch ', epoch, ': ', avg_train_loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model ...\n",
            "  Batch    40  of  5,541.    Elapsed: 27.321417570114136.\n",
            "  Batch    80  of  5,541.    Elapsed: 54.523813247680664.\n",
            "  Batch   120  of  5,541.    Elapsed: 81.74624466896057.\n",
            "  Batch   160  of  5,541.    Elapsed: 108.94300580024719.\n",
            "  Batch   200  of  5,541.    Elapsed: 136.11745834350586.\n",
            "  Batch   240  of  5,541.    Elapsed: 163.31518268585205.\n",
            "  Batch   280  of  5,541.    Elapsed: 190.49569129943848.\n",
            "  Batch   320  of  5,541.    Elapsed: 217.6815013885498.\n",
            "  Batch   360  of  5,541.    Elapsed: 244.87786674499512.\n",
            "  Batch   400  of  5,541.    Elapsed: 272.0622236728668.\n",
            "  Batch   440  of  5,541.    Elapsed: 299.26291060447693.\n",
            "  Batch   480  of  5,541.    Elapsed: 326.4547119140625.\n",
            "  Batch   520  of  5,541.    Elapsed: 353.62776136398315.\n",
            "  Batch   560  of  5,541.    Elapsed: 380.79663825035095.\n",
            "  Batch   600  of  5,541.    Elapsed: 407.9691915512085.\n",
            "  Batch   640  of  5,541.    Elapsed: 435.1525454521179.\n",
            "  Batch   680  of  5,541.    Elapsed: 462.3153042793274.\n",
            "  Batch   720  of  5,541.    Elapsed: 489.5031363964081.\n",
            "  Batch   760  of  5,541.    Elapsed: 516.6803393363953.\n",
            "  Batch   800  of  5,541.    Elapsed: 543.8654825687408.\n",
            "  Batch   840  of  5,541.    Elapsed: 571.0468440055847.\n",
            "  Batch   880  of  5,541.    Elapsed: 598.218409538269.\n",
            "  Batch   920  of  5,541.    Elapsed: 625.4034624099731.\n",
            "  Batch   960  of  5,541.    Elapsed: 652.5694754123688.\n",
            "  Batch 1,000  of  5,541.    Elapsed: 679.7357933521271.\n",
            "  Batch 1,040  of  5,541.    Elapsed: 706.9096219539642.\n",
            "  Batch 1,080  of  5,541.    Elapsed: 734.1073670387268.\n",
            "  Batch 1,120  of  5,541.    Elapsed: 761.2718553543091.\n",
            "  Batch 1,160  of  5,541.    Elapsed: 788.430876493454.\n",
            "  Batch 1,200  of  5,541.    Elapsed: 815.6053831577301.\n",
            "  Batch 1,240  of  5,541.    Elapsed: 842.8070974349976.\n",
            "  Batch 1,280  of  5,541.    Elapsed: 870.016294002533.\n",
            "  Batch 1,320  of  5,541.    Elapsed: 897.2275125980377.\n",
            "  Batch 1,360  of  5,541.    Elapsed: 924.4124307632446.\n",
            "  Batch 1,400  of  5,541.    Elapsed: 951.6196284294128.\n",
            "  Batch 1,440  of  5,541.    Elapsed: 978.8216788768768.\n",
            "  Batch 1,480  of  5,541.    Elapsed: 1006.0252432823181.\n",
            "  Batch 1,520  of  5,541.    Elapsed: 1033.2208979129791.\n",
            "  Batch 1,560  of  5,541.    Elapsed: 1060.4193913936615.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMW0lX06Z2fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the trained model (Create the directory if it does not exist; otherwise override the contents)\n",
        "import os\n",
        "if not os.path.exists('./saved_model'):\n",
        "    os.makedirs('./saved_model')\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained('./saved_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq984coKin2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_file = './dev-v1.1.json'\n",
        "dev_examples = read_squad_examples(dev_file, is_training = False, version_2_with_negative=False)\n",
        "\n",
        "print(\"total evaluation samples = \", len(dev_examples))\n",
        "dev_examples[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6M2i7kmioQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_features = convert_examples_to_features(dev_examples, tokenizer, max_seq_length=384, doc_stride=128, max_query_length=64, is_training=False)\n",
        "len(dev_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnMNFNetiob-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert dev tokenized data into torch tensors\n",
        "all_input_ids = torch.tensor([f.input_ids for f in dev_features], dtype = torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in dev_features], dtype = torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in dev_features], dtype = torch.long)\n",
        "all_example_index = torch.arange(all_input_ids.size(0), dtype = torch.long)\n",
        "\n",
        "all_input_ids[1], all_input_mask[1], all_segment_ids[1], all_example_index[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzaLY7LjioZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kmGxaB0YJz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Validation of the model on dev dataset \n",
        "\n",
        "print(\"Validating the model...\")\n",
        "model.eval()\n",
        "results = []\n",
        "for input_ids, input_masks, segment_ids, example_indices in dev_dataloader:\n",
        "  input_ids = input_ids.to(device)\n",
        "  input_masks = input_masks.to(device)    \n",
        "  segment_ids = segment_ids.to(device)    \n",
        "  with torch.no_grad():\n",
        "    batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_masks)\n",
        "  for i, example_index in enumerate(example_indices):\n",
        "    start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
        "    end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
        "    dev_feature = dev_features[example_index.item()]\n",
        "    unique_id = int(dev_feature.unique_id)\n",
        "    rawResult = RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits)\n",
        "    results.append(rawResult)\n",
        "        \n",
        "if not os.path.exists('./model_eval'):\n",
        "    os.makedirs('./model_eval')\n",
        "output_prediction_file = os.path.join(\"./model_eval\", \"predictions.json\")\n",
        "output_nbest_file = os.path.join(\"/model_eval\", \"nbest_predictions.json\")\n",
        "output_null_log_odds_file = os.path.join(\"./model_eval\", \"null_odds.json\")\n",
        "\n",
        "preds = write_predictions(dev_examples, dev_features, results, 20, 30, True, output_prediction_file, output_nbest_file, output_null_log_odds_file, True, False, 0.0)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41lAYqAatprg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}